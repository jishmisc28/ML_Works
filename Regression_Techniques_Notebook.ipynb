{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Techniques in Machine Learning \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prerequisites__\n",
    "\n",
    "*Anaconda\n",
    "    - Most of the essential prepackaged libraries\n",
    "*Pycharm\n",
    "    - IDE (IntelliJ)\n",
    "*Win/Linux PC with DualCore or higher\n",
    "*GPU -AMD/Intel Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Regression?\n",
    "   Regression is a parametric technique used to predict continuous (dependent) variable given a set of independent variables. <br><br>\n",
    "   It is parametric in nature because it makes certain assumptions (discussed next) based on the data set. If the data set follows those assumptions, regression gives incredible results. Otherwise, it struggles to provide convincing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). <br><br>\n",
    "More specifically, that y can be calculated from a linear combination of the input variables (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, regression uses a linear function to approximate (predict) the dependent variable given as:\n",
    "\n",
    " <br> **Y = βo + β1X + ∈**       <br>\n",
    "\t\n",
    "   It's called 'linear' because there is just one independent variable (X) involved. \n",
    "           (∈ - This represents the residual value, i.e. the difference between actual and predicted values.)\n",
    "\t\t\n",
    "Elimination of the error (∈) is most often not possible in practical scenarios hence, we use a technique called Ordinary Least Square (OLS).\n",
    "\t\t\n",
    "Conceptually, OLS technique tries to reduce the sum of squared errors ∑[Actual(y) - Predicted(y')]² by finding the best possible value of regression coefficients (β0, β1, etc).\n",
    "\t\t\n",
    "\t\t\t\tβ1 = Σ(xi - xmean)(yi-ymean)/ Σ (xi - xmean)² where i= 1 to n (no. of obs.)\n",
    "                βo = ymean - β1(xmean)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"regularization_notebook01.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the assumptions made in regression ?\n",
    "\n",
    "* There exists a linear and additive relationship between dependent (DV) and independent variables (IV).\n",
    "* There must be no correlation among independent variables. Presence of correlation in independent variables lead to Multicollinearity. If variables are correlated, it becomes extremely difficult for the model to determine the true effect of IVs on DV.\n",
    "* The error terms must possess constant variance. Absence of constant variance leads to heteroskedestacity.\n",
    "* The error terms must be uncorrelated i.e. error at ∈t must not indicate the at error at ∈t+1. Presence of correlation in error terms is known as Autocorrelation. It drastically affects the regression coefficients and standard error values since they are based on the assumption of uncorrelated error terms.\n",
    "* The dependent variable and the error terms must possess a normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are these “residuals” everyone keeps talking about?\n",
    "\n",
    "A residual is just how far off a model is for a single point. So if our model predicts that a 20 pound cantaloupe should sell for eight dollars and it actually sells for ten dollars, the residual for that data point would be two dollars. Most models will be off by at least a little bit for pretty much all points, but you want to make sure that there’s not a strong pattern in your residuals because that suggests that your model is failing to capture some underlying trend in your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Plots \n",
    "=======================\n",
    "1. __Residual vs. Fitted Values Plot__: This plot shows if residuals have non-linear patterns. Ideally, this plot shouldn't show any pattern. But if you see any shape (curve, U shape, funnel), it suggests non-linearity in the data set.\n",
    "\n",
    "2. __Normality Q-Q Plot__: This plot shows if residuals are normally distributed. It’s good if residuals are lined well on the straight dashed line. Basically, this plot is used to determine the normal distribution of errors.\n",
    "\n",
    "3. __Scale Location Plot__: It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.\n",
    "\n",
    "4. __Residuals vs Leverage__: This plot helps us to find influential cases if any. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook’s distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"regression_notebook01.gif\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Median Mode Variance & Std.Deviation\n",
    "_________\n",
    "\n",
    "__Mean__:The average value of a sample of population.<br>\n",
    "__Median__:One problem with mean is it does not depict the true outcome if their is skewness/outliers in data. The alternative measure is median. The median is the middle score.<br>\n",
    "__Mode__:The mode of a set of data is the number with the highest frequency.<br>\n",
    "__Variance__:If we wish to know how far the data/distribution is spread apart that's where variance and std.deviation helps.<br>\n",
    "__Std.Deviation__:The standard deviation can be thought of measuring how far the data values lie from the mean.<br>   \n",
    "    \n",
    "   __ Variance and Standard Deviation: Step by Step __\n",
    "\n",
    "    1. Calculate the mean, x.\n",
    "\n",
    "    2. Write a table that subtracts the mean from each observed value.\n",
    "\n",
    "    3. Square each of the differences.\n",
    "\n",
    "    4. Add this column.\n",
    "\n",
    "    5. Divide by n -1 where n is the number of items in the sample  This is the variance.\n",
    "\n",
    "    6. To get the standard deviation we take the square root of the variance.  \n",
    "\n",
    "    \n",
    "### __Normal Distribution__\n",
    "________\n",
    "A normal distribution is defined by the probability density function and it follows a bell curve and denotes the distribution that occurs naturally in many situations. \n",
    "\t\t\t\n",
    "\t\t\tHeights of people.\n",
    "\t\t\tMeasurement errors.\n",
    "\t\t\tBlood pressure.\n",
    "\t\t\tPoints on a test.\n",
    "\t\t\tIQ scores.\n",
    "\t\t\tSalaries.\n",
    "The empirical rule tells you what percentage of your data falls within a certain number of standard deviations from the mean:\n",
    "\t\t• 68% of the data falls within one standard deviation of the mean.\n",
    "\t\t• 95% of the data falls within two standard deviations of the mean.\n",
    "\t\t• 99.7% of the data falls within three standard deviations of the mean.\n",
    "\t\t• The total area under the curve is 1.\n",
    "\n",
    "\t\tWe can take any Normal Distribution and convert it to The Standard Normal Distribution.\n",
    "\t\t\t- In standard normal distribution, the mean () is zero and std. deviation is 1. \n",
    "\t\t\t-  This is done through calculations of a z-score.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of regression \n",
    "\n",
    "There are many different types of regression. All of the types of regressions we’ll be learning are called “generalized linear models”. \n",
    "\n",
    "•Linear: When you’re predicting a continuous value. (What temperature will it be today?)<br>\n",
    "•Logistic: When you’re predicting which category your observation is in. (Is this is a cat or a dog?)<br>\n",
    "•Poisson: When you’re predicting a count value. (How many dogs will I see in the park?)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Regression Analysis Output (“Goodness of Fit” Measures)\n",
    "_____________________\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\tCall: lm(formula=weight ~ height, data=women)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tResiduals:\n",
    "\t\t\t\t\t  Min \t 1Q\t Median   3Q    Max\n",
    "\t\t\t\t\t-1.733  -1.133  -0.383    0.742   3.117\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tCoefficients:\n",
    "\t\t\t\t\t\t\t\tEstimate | Std.Error | tvalue | Pr(>|t|)    |\n",
    "\t\t\t\t\t(Intercept) -87.5167 |  5.9369   | -14.7  | 1.7e-09 *** |\n",
    "\t\t\t\t\theight       3.4500  | 0.0911    | 37.9   | 1.1e-14 *** |\n",
    "\t\t\t\t\t---\n",
    "\t\t\t\t\tSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tResidual standard error: 1.53 on 13 degrees of freedom\n",
    "\t\t\t\t\tMultiple R-squared: 0.991, Adjusted R-squared: 0.99\n",
    "\t\t\t\t\tF-statistic: 1.43e+03 on 1 and 13 DF, p-value: 1.09e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can you access the fit of regression model?\n",
    "\t\t\n",
    "1. __R Square (Coefficient of Determination)__ - It tells you how many points fall on the regression line.For example, 80% means that 80% of the values fit the model.\n",
    "\t\t\t\n",
    "2. __Adjusted R²__ - The problem with R² is that it keeps on increasing as you increase the number of variables, regardless of the fact that the new variable is actually adding new information to the model. To overcome that, we use adjusted R² which doesn't increase (stays same or decrease) unless the newly added variable is truly useful.\n",
    "\t\t\t\n",
    "3. __F Statistics__ - It evaluates the overall significance of the model. It is the ratio of explained variance by the model by unexplained variance. It compares the full model with an intercept only (no predictors) model. Its value can range between zero and any arbitrary large number. Naturally, higher the F statistics, better the model.\n",
    "\t\t\t\n",
    "4. __RMSE__ - This is root mean square error. It is interpreted as how far on an average, the residuals are from zero. It nullifies squared effect of MSE by square root and provides the result in original units as data. Suppose the actual y is 10 and predictive y is 30, the resultant RMSE would be √(30-10)² = 20. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Deviance) Residuals__: You can pretty much ignore these for logistic regression. For Poisson or logistic regression, you want these to be more-or-less normally distributed (which is the same thing the top two diagnostic plots are checking). You can check this by seeing if the absolute value of 1Q and 3Q are close(ish) to each other, and if the median is close to 0. The mean is not shown because it's always 0. If any of these are super off then you probably have some weird skew in your data. (This will also show up in your diagnostic plots!)<br>\n",
    "\n",
    "__Coefficients:__ This is the meat of the output.<br>\n",
    "\n",
    "__•Intercept:__ For Poisson and linear regression, this is the predicted output when all our inputs are 0. For logistic regression, this value will be further away from 0 the bigger the difference between the number of observation in each class.. The the standard error represents how uncertain we are about this (lower is better). In this case, because our intercept is far from 0 and our standard error is much smaller than the intercept, we can be pretty sure that one of our classes (failed or didn't fail) has a lot of more observations in it. (In this case it's \"didn't fail\", thankfully!)<br>\n",
    "__•Various inputs (each input will be on a different line):__ This estimate represents how much we think the output will change each time we increase this input by 1. The bigger the estimate, the bigger the effect of this input variable on the output. The standard error is how certain about it we are. Usually, we can be pretty sure an input is informative is the standard error is 1/10 of the estimate. So in this case we're pretty sure the intercept is important.<br>\n",
    "__•Signif. Codes:__ This is a key to the significance of each :input and the intercept. These are only correct if you only ever fit one model to your data. (In other words, they’re great for experimental data if you from the start which variables you’re interested in and not as informative for data analysis or variable selection.)<br>\n",
    "\n",
    "__Null deviance:__ The null deviance tells us how well we can predict our output only using the intercept. Smaller is better.<br>\n",
    "\n",
    "__Residual deviance:__ The residual deviance tells us how well we can predict our output using the intercept and our inputs. Smaller is better. The bigger the difference between the null deviance and residual deviance is, the more helpful our input variables were for predicting the output variable.<br>\n",
    "\n",
    "__AIC:__ The AIC is the \"Akaike information criterion\" and it's an estimate of how well your model is describing the patterns in your data. It's mainly used for comparing models trained on the same dataset. If you need to pick between models, the model with the lower AIC is doing a better job describing the variance in the data.<br>\n",
    "\n",
    "__Number of Fisher Scoring iterations:__ This is just a measure of how long it took to fit you model. You can safely ignore it.<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
